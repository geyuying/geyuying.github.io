<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<link rel="shortcut icon" href="myIcon.ico">
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />

<meta name="keywords" content="Yuying Ge, CS, HKU, The University of Hong Kong">
<meta name="description" content="Yuying Ge&#39;s home page">
<link rel="stylesheet" href="jemdoc.css" type="text/css">
<title>Yuying Ge&#39;s Homepage</title>
<!-- Google Analytics -->
<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-159069803-1', 'auto');
ga('send', 'pageview');
</script>
<!-- End Google Analytics -->
<!--
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-87320911-1', 'auto');
  ga('send', 'pageview');

</script>
-->
</head>
<body>

<div id="layout-content" style="margin-top:25px">
<table>
	<tbody>
		<tr>
			<td width="670">
				<div id="toptitle">
					<h1>Yuying Ge <font face="Arial">    葛玉莹 </font></h1></div>
				<img src="./pic/others/email.png" height="22px">  yyge13@gmail.com<br>
				<br>
				<img src="./pic/others/google_scholar_logo.png" height="22px">  <a href="https://scholar.google.com/citations?user=hv1LiiEAAAAJ&hl=en">Google Sholar</a><br>
				<br>
				<img src="./pic/others/github_logo.png" height="22px">  <a href="https://github.com/geyuying">Github</a><br>
				<br>
				<img src="./pic/others/location.png" height="22px"> Shenzhen, China<br>

			</td>
			<td>
    <img src="./pic/yuying.jpeg" border="0" width="180"><br>
   </td>
		</tr><tr>
	</tr></tbody>
</table>

<!--<h2>Biography [<a href="./CV-JinYueming.pdf">CV</a>]</h2>-->
<h2>Biography </h2>
<p>
	I am currently a Senior Researcher at <a href="https://arc.tencent.com/en/index">Tencent ARC Lab</a>, working on multimodal foundation models.
	Before that, I was a reseacher at <a href="https://ai.tencent.com/ailab/en/index/">Tencent AI Lab</a>.
	In Aug 2023, I got my Ph.D. degree from the Department of Computer Science, The University of Hong Kong,
	under the supervision of <a href="http://luoping.me/">Prof. Ping Luo</a>.
	 I was also a visiting student at UCSD, working with <a href="https://xiaolonw.github.io/">Prof. Xiaolong Wang</a>.
	We are actively looking for self-motivated interns to work on related research topics. Please feel free to reach out if you are interested.

	<br>
</p>


<h2>News</h2>
<ul>
				<li>
		[12/2024] We release <a href = "https://arxiv.org/abs/2412.04432" target="_blank">Divot</a>, a diffusion-powered video tokenizer for unified comprehension and generation.
						</li>
				<li>
		[12/2024] We release <a href = "https://arxiv.org/abs/2412.04446" target="_blank">DiCoDe</a> for autoregressive video generation with LLMs.
						</li>
				<li>
		[12/2024] We release <a href = "https://arxiv.org/abs/2412.04445" target="_blank">MoTo</a>, Latent Motion Tokens for autoregressive video pretraining to enhance robot manipulation.
	</li>
				<li>
		[12/2024] We release <a href = "https://arxiv.org/abs/2412.04447" target="_blank">EgoPlan-Bench2</a>, evaluating planning capabilities of MLLMs across various real-world scenarios.
	</li>
				<li>
		[07/2024] We release <a href = "https://arxiv.org/abs/2407.08683" target="_blank">SEED-Story</a> for Multimodal Long Story Generation based on SEED-X.
	</li>
				<li>
		[04/2024] We release <a href = "https://arxiv.org/abs/2404.14396" target="_blank">SEED-X</a>, the latest in our SEED series, which unifies multi-granularity comprehension and generation.
	</li>
				<li>
		[02/2024] <a href = "https://github.com/AILab-CVC/SEED-Bench" target="_blank">SEED-Bench</a> is accepted by CVPR 2024.
	</li>
			<li>
		[01/2024] <a href = "https://arxiv.org/abs/2310.01218" target="_blank">SEED-LLaMA</a> is accepted by ICLR 2024.
	</li>
		<li>
		[12/2023] We release  <a href = "https://chenyi99.github.io/ego_plan" target="_blank">EgoPlan-Bench</a>, which evaluates egocentric embodied planning of MLLMs.
		</li>
		<li>
		[11/2023] We release  <a href = "https://github.com/AILab-CVC/SEED-Bench" target="_blank">SEED-Bench-2</a>, evaluating the hierarchical capabilities of MLLMs.
		</li>
		<li>
		[10/2023] We release an online gradio demo of <a href = "https://10a4e7976e6fc2032c.gradio.live/" target="_blank">SEED-LLaMA</a>.
		</li>
		<li>
		[10/2023] We release the technical report of <a href = "https://arxiv.org/abs/2310.01218" target="_blank">SEED-LLaMA</a>, which is empowered by the improved SEED-2 tokenizer.
	</li>
		<li>
		[08/2023] We release <a href = "https://github.com/AILab-CVC/SEED-Bench" target="_blank">SEED-Bench</a>, the most comprehensive MLLM benchmark to date.
	</li>
		<li>
		[07/2023] We release our <a href = "https://github.com/AILab-CVC/SEED" target="_blank">SEED</a>. Stay tuned for more updates.
	</li>
	<li>
		[02/2023] Three papers were accepted by CVPR 2023.
	</li>
	<li>
		[07/2022] One paper was accepted by ECCV 2022.
	</li>
	<li>
		[03/2022] One paper was accepted by CVPR 2022 as oral.
	</li>
	<li>
		[11/2021] One paper was accepted by IEEE TIP.
	</li>
	<li>
		[03/2021] Two papers were accepted by CVPR 2021.
	</li>
	<li>
		[03/2019] One paper was accepted by CVPR 2019.
	</li>
</ul>




<h2> Selected Publications</h2>
( *equal contribution   <sup>#</sup>corresponding author / project lead )
<ul>
	<li>
		Divot: Diffusion Powers Video Tokenizer for Comprehension and Generation <br />
		<b>Yuying Ge</b>, Yizhuo Li, Yixiao Ge, Ying Shan
		<br /> arXiv preprint, 2024.
		[<a href="https://arxiv.org/abs/2412.04432">Paper</a>]
		[<a href="https://github.com/TencentARC/Divot">Code</a>]
		<img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/TencentARC/Divot?style=social">
		<br />
	  </li>
	<li>
		DiCoDe: Diffusion-Compressed Deep Tokens for Autoregressive Video Generation with Language Models <br />
		Yizhuo Li, <b>Yuying Ge<sup>#</sup></b>, Yixiao Ge, Ping Luo, Ying Shan
		<br /> arXiv preprint, 2024.
		[<a href="https://arxiv.org/abs/2412.04446">Paper</a>]
		[<a href="https://liyizhuo.com/DiCoDe/">Project</a>]
		<br />
	  </li>
	<li>
		Moto: Latent Motion Token as the Bridging Language for Robot Manipulation <br />
		Yi Chen, <b>Yuying Ge<sup>#</sup></b>, Yizhuo Li, Yixiao Ge, Mingyu Ding, Ying Shan, Xihui Liu
		<br /> arXiv preprint, 2024.
		[<a href="https://arxiv.org/pdf/2412.04445">Paper</a>]
		[<a href="https://github.com/TencentARC/Moto/">Code</a>]
		[<a href="https://chenyi99.github.io/moto/">Project</a>]
		<img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/TencentARC/Moto?style=social">
		<br />
	  </li>
	<li>
		EgoPlan-Bench2: A Benchmark for Multimodal Large Language Model Planning in Real-World Scenarios <br />
		Lu Qiu, <b>Yuying Ge<sup>#</sup></b>, Yi Chen, Yixiao Ge, Ying Shan, Xihui Liu
		<br /> arXiv preprint, 2024.
		[<a href="https://arxiv.org/abs/2412.04447">Paper</a>]
		[<a href="https://github.com/qiulu66/EgoPlan-Bench2/">Code</a>]
		[<a href="https://qiulu66.github.io/egoplanbench2/">Project</a>]
		<img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/qiulu66/EgoPlan-Bench2?style=social">
		<br />
	  </li>
	<li>
		SEED-Story: Multimodal Long Story Generation with Large Language Model <br />
		Shuai Yang, <b>Yuying Ge<sup>#</sup></b>, Yang Li, Yukang Chen, Yixiao Ge, Ying Shan, Yingcong Chen
		<br /> arXiv preprint, 2024.
		[<a href="https://arxiv.org/abs/2407.08683">Paper</a>]
		[<a href="https://github.com/TencentARC/SEED-Story">Code</a>]
		<img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/TencentARC/SEED-Story?style=social">
		<br />
	  </li>
	<li>
		EgoPlan-Bench: Benchmarking Multimodal Large Language Models for Human-Level Planning <br />
		Yi Chen, <b>Yuying Ge<sup>#</sup></b>, Yixiao Ge, Mingyu Ding, Bohao Li, Rui Wang, Ruifeng Xu, Ying Shan, Xihui Liu
		<br /> arXiv preprint, 2024.
		[<a href="https://arxiv.org/abs/2404.14396">Paper</a>]
		[<a href="https://github.com/ChenYi99/EgoPlan">Code</a>]
		[<a href="https://chenyi99.github.io/ego_plan/">Project</a>]
		<img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/ChenYi99/EgoPlan?style=social">
		<br />
	  </li>
	<li>
		SEED-X: Multimodal Models with Unified Multi-granularity Comprehension and Generation <br />
		<b>Yuying Ge*</b>, Sijie Zhao*, Jinguo Zhu*, Yixiao Ge, Kun Yi, Lin Song, Chen Li, Xiaohan Ding, Ying Shan
		<br /> arXiv preprint, 2024.
		[<a href="https://arxiv.org/abs/2404.14396">Paper</a>]
		[<a href="https://github.com/AILab-CVC/SEED-X">Code</a>]
		[<a href="https://arc.tencent.com/en/ai-demos/multimodal">Demo</a>]
		<img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/AILab-CVC/SEED-X?style=social">
		<br />
	  </li>
	  <li>
		SEED-Bench: Benchmarking Multimodal Large Language Models <br />
		Bohao Li*, <b>Yuying Ge*</b>, Yixiao Ge, Guangzhi Wang, Rui Wang, Ruimao Zhang, Ying Shan
		<br /> CVPR, 2024.
		[<a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Li_SEED-Bench_Benchmarking_Multimodal_Large_Language_Models_CVPR_2024_paper.pdf">Paper</a>]
		[<a href="https://github.com/AILab-CVC/SEED-Bench">Code</a>]
		[<a href="https://huggingface.co/datasets/AILab-CVC/SEED-Bench-2">Data</a>]
		[<a href="https://huggingface.co/spaces/AILab-CVC/SEED-Bench_Leaderboard">Leaderboard</a>]
		<img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/AILab-CVC/SEED-Bench?style=social">
		<br />
	  </li>
	  <li>
		Making LLaMA SEE and Draw with SEED Tokenizer <br />
		<b>Yuying Ge*</b>, Sijie Zhao*, Ziyun Zeng, Yixiao Ge, Chen Li, Xintao Wang, Ying Shan
		<br /> ICLR, 2024.
		[<a href="https://openreview.net/forum?id=0Nui91LBQS">Paper</a>]
		[<a href="https://github.com/AILab-CVC/SEED/tree/main">Code</a>]
		<img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/AILab-CVC/SEED?style=social">
		<br />
	  </li>
	  <li>
		Planting a SEED of Vision in Large Language Model <br />
		<b>Yuying Ge*</b>, Yixiao Ge*, Ziyun Zeng, Xintao Wang, Ying Shan
		<br /> Technical Report, 2023.
		[<a href="https://arxiv.org/abs/2307.08041">Paper</a>]
		[<a href="https://github.com/AILab-CVC/SEED/tree/v1">Code</a>]
		<img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/AILab-CVC/SEED?style=social">
		<br />
	  </li>
	  <li>
		Policy Adaptation from Foundation Model Feedback <br />
		<b>Yuying Ge</b>, Annabella Macaluso, Li Erran Li, Ping Luo, Xiaolong Wang
		<br /> CVPR, 2023.
		[<a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Ge_Policy_Adaptation_From_Foundation_Model_Feedback_CVPR_2023_paper.pdf">Paper</a>]
		[<a href="ttps://geyuying.github.io/PAFF/">Project</a>]
		[<a href="https://github.com/geyuying/PAFF_code">Code</a>]
		<img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/geyuying/PAFF_code?style=social">
		<br />
	  </li>
	  <li>
		Learning Transferable Spatiotemporal Representations from Natural Script Knowledge <br />
		Ziyun Zeng*, <b>Yuying Ge*</b>, Xihui Liu, Bin Chen, Ping Luo, Shu-Tao Xia, Yixiao Ge
		<br /> CVPR, 2023.
		[<a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zeng_Learning_Transferable_Spatiotemporal_Representations_From_Natural_Script_Knowledge_CVPR_2023_paper.pdf">Paper</a>]
		[<a href="https://github.com/TencentARC/TVTS">Code</a>]
		<img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/TencentARC/TVTS?style=social">
		<br />
	  </li>
	  <li>
		MILES: Visual BERT Pre-training with Injected Language Semantics for Video-text Retrieval <br />
		<b>Yuying Ge</b>, Yixiao Ge, Xihui Liu, Alex Jinpeng Wang, Jianping Wu, Ying Shan, Xiaohu Qie and Ping Luo
		<br /> ECCV, 2022.
		[<a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136950685.pdf">Paper</a>]
		[<a href="https://github.com/TencentARC/MCQ/blob/main/MILES.md">Code</a>]
		<img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/TencentARC/MCQ?style=social">
		<br />
	  </li>
	  <li>
		Bridging Video-text Retrieval with Multiple Choice Questions <br />
		<b>Yuying Ge</b>, Yixiao Ge, Xihui Liu, Dian Li, Ying Shan, Xiaohu Qie and Ping Luo</i>
		<br /> CVPR, 2022 (oral).
		[<a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Ge_Bridging_Video-Text_Retrieval_With_Multiple_Choice_Questions_CVPR_2022_paper.pdf">Paper</a>]
		[<a href="MCQ.html">Project</a>]
		[<a href="https://github.com/TencentARC/MCQ">Code</a>]
		<img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/TencentARC/MCQ?style=social">
		<br />
	  </li>
	  <li>
		Parser-Free Virtual Try-on via Distilling Appearance Flows <br />
		<b>Yuying Ge</b>, Yibing Song, Ruimao Zhang, Chongjian Ge, Wei Liu, and Ping Luo
		<br /> CVPR, 2021.
		[<a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Ge_Parser-Free_Virtual_Try-On_via_Distilling_Appearance_Flows_CVPR_2021_paper.pdf">Paper</a>]
		[<a href="https://github.com/geyuying/PF-AFN">Code</a>]
		<img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/geyuying/PF-AFN?style=social">
		<br />
	  </li>
	  <li>
		DeepFashion2: A Versatile Benchmark for Detection, Pose Estimation, Segmentation and Re-Identification of Clothing Images <br />
		<b>Yuying Ge</b>, Ruimao Zhang, Xiaogang Wang, Xiaoou Tang, and Ping Luo
		<br /> CVPR, 2019.
		[<a href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Ge_DeepFashion2_A_Versatile_Benchmark_for_Detection_Pose_Estimation_Segmentation_and_CVPR_2019_paper.pdf">Paper</a>]
		[<a href="https://github.com/switchablenorms/DeepFashion2">Data</a>]
		<img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/switchablenorms/DeepFashion2?style=social">
		<br />
	  </li>
	</ul>





</tbody></table>

	<h2><font> Education </font></h2>
<ul style="list-style-type:none">
	  <p style="margin-left: 0px; line-height: 150%; margin-top: 8px; margin-bottom: 8px;"><font size="3"><meta charset="utf-8">
		  Ph.D., Department of Computer Science, The University of Hong Kong, 2019 - 2023<br>
		  Bachelor, University of Electronic Science and Technology of China (UESTC) (ranking 1/525), 2014 - 2018<br>
	  </font> </p>
</ul>

	<h2><font> Experiences </font></h2>
<ul style="list-style-type:none">
	  <p style="margin-left: 0px; line-height: 150%; margin-top: 8px; margin-bottom: 8px;"><font size="3"><meta charset="utf-8">
		  Senior Researcher in Tencent ARC Lab, 2024 - Present<br>
		  Senior Researcher in Tencent AI Lab, 2023 - 2024<br>
		  Intern in Tencent ARC Lab, 2021 - 2022 <br>
		  Intern in Tencent AI Lab, 2020 - 2021 <br>
		  Research Assistant in Multimedia Lab (MMLab), The Chinese University of Hong Kong, 2018 - 2019 <br>
		  Intern in SenseTime Research, 2017 - 2018 <br>

	  </font> </p>
</ul>

<h2><font> Academic Activities </font></h2>
<ul style="list-style-type:none">
	  <p style="margin-left: 0px; line-height: 150%; margin-top: 8px; margin-bottom: 8px;"><font size="3"><meta charset="utf-8">
		  Reviewer for CVPR, ICLR, ICML, NeurIPS, ECCV, ICCV, TPAMI, TNNLS, TMM, TVCJ<br>
		  Organizer of DeepFashion2 Challenge <a href='https://competitions.codalab.org/competitions/22966'>Clothes Landmark Detection</a>
			  and <a href='https://competitions.codalab.org/competitions/22967'>Clothes Retrieval</a> in 2019, 2020<br>
		  Organizer of <a href='https://sites.google.com/view/cvcreative2020'>Third Workshop on Computer Vision for Fashion, Art and Design</a> in CVPR, 2020 <br>
		  Organizer of <a href='https://sites.google.com/view/cvcreative/home?authuser=0'>Second Workshop on Computer Vision for Fashion, Art and Design</a> in ICCV, 2019 <br>
	  </font> </p>
</ul>




<p align=right>
	<a class="pull-right" href="#">
	<script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=zPFGAwps3dHeHg_L_f7k-PGV37THUtOVbgXT6ZedWOs&cl=ffffff&w=a"></script></center>
	</a>
</p>

<p><center><font>
        <br>&copy; Yuying Ge | Last updated: Dec. 2021</font></center>
</p>

</div>
</body></html>
