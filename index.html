<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<link rel="shortcut icon" href="myIcon.ico">
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />

<meta name="keywords" content="Yuying Ge, CS, HKU, The University of Hong Kong">
<meta name="description" content="Yuying Ge&#39;s home page">
<link rel="stylesheet" href="jemdoc.css" type="text/css">
<title>Yuying Ge&#39;s Homepage</title>
<!-- Google Analytics -->
<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-159069803-1', 'auto');
ga('send', 'pageview');
</script>
<!-- End Google Analytics -->
<!--
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-87320911-1', 'auto');
  ga('send', 'pageview');

</script>
-->
</head>
<body>

<div id="layout-content" style="margin-top:25px">
<table>
	<tbody>
		<tr>
			<td width="670">
				<div id="toptitle">
					<h1>Yuying Ge <font face="Arial">    葛玉莹 </font></h1></div>
				<img src="./pic/others/email.png" height="22px">  yyge13@gmail.com<br>
				<br>
				<img src="./pic/others/google_scholar_logo.png" height="22px">  <a href="https://scholar.google.com/citations?user=hv1LiiEAAAAJ&hl=en">Google Sholar</a><br>
				<br>
				<img src="./pic/others/github_logo.png" height="22px">  <a href="https://github.com/geyuying">Github</a><br>
				<br>
				<img src="./pic/others/location.png" height="22px"> Shenzhen, China<br>

			</td>
			<td>
    <img src="./pic/yuying.jpeg" border="0" width="180"><br>
   </td>
		</tr><tr>
	</tr></tbody>
</table>

<!--<h2>Biography [<a href="./CV-JinYueming.pdf">CV</a>]</h2>-->
<h2>Biography </h2>
<p>
	I am currently a Staff Research Scientist at <a href="https://www.xpeng.com/">XPENG Robotics</a>, exploring cutting-edge technologies of embodied AI. Previously, I spent two wonderful years at 
	<a href="https://arc.tencent.com/en/index">Tencent ARC Lab</a> and <a href="https://ai.tencent.com/ailab/en/index/">Tencent AI Lab</a>, working on multimodal foundation models.
	In Aug 2023, I got my Ph.D. degree from the Department of Computer Science, The University of Hong Kong,
	under the supervision of <a href="http://luoping.me/">Prof. Ping Luo</a>.
	 I was also a visiting student at UCSD, working with <a href="https://xiaolonw.github.io/">Prof. Xiaolong Wang</a>.
	We are actively looking for full-time researchers, engineers and interns to work on related topics. Please feel free to reach out if you are interested.

	<br>
</p>


<h2>News</h2>
<ul>
				<li>
		[07/2025] We release <a href = "https://arxiv.org/abs/2507.20939" target="_blank">ARC-Hunyuan-Video</a>, a multimodal model for structured comprehension of real-world short videos.
						</li>
				<li>
		[06/2025] We release <a href = "https://arxiv.org/abs/2506.16141" target="_blank">GRPO-CARE</a>, introducing consistency-aware GRPO for multimodal reasoning.
						</li>
				<li>
		[06/2025] We release <a href = "https://arxiv.org/abs/2506.05240" target="_blank">FlowAlign</a>, using flow-based models to align learnable latent spaces to target distributions.
						</li>
				<li>
		[06/2025] We release <a href = "https://arxiv.org/abs/2506.03126" target="_blank">AnimeShooter</a>, multi-shot animation dataset for reference-guided video generation.
						</li>
				<li>
		[05/2025] We release <a href = "https://arxiv.org/abs/2505.21374" target="_blank">Video-Holmes</a>, evaluating MLLMs for complex video reasoning like Holmes.
						</li>
				<li>
		[04/2025] We release <a href = "https://arxiv.org/abs/2504.01014" target="_blank">AnimeGamer</a>, transforming characters from anime films into interactive entities with a MLLM.
						</li>
				<li>
		[04/2025] We release <a href = "https://arxiv.org/abs/2503.24376" target="_blank">SEED-Bench-R1</a>, exploring the effects of RL on video understanding with hierarchical evaluation.
						</li>
				<li>
		[03/2025] We release <a href = "https://arxiv.org/abs/2503.19480" target="_blank">GenHancer</a>, using imperfect generative models for enhanced vision-centric representations.
						</li>
				<li>
		[12/2024] We release <a href = "https://arxiv.org/abs/2412.04432" target="_blank">Divot</a>, a diffusion-powered video tokenizer for unified comprehension and generation.
						</li>
				<li>
		[12/2024] We release <a href = "https://arxiv.org/abs/2412.04446" target="_blank">DiCoDe</a> for autoregressive video generation with LLMs.
						</li>
				<li>
		[12/2024] We release <a href = "https://arxiv.org/abs/2412.04445" target="_blank">MoTo</a>, Latent Motion Tokens for autoregressive video pretraining to enhance robot manipulation.
	</li>
				<li>
		[12/2024] We release <a href = "https://arxiv.org/abs/2412.04447" target="_blank">EgoPlan-Bench2</a>, evaluating planning capabilities of MLLMs across various real-world scenarios.
	</li>
				<li>
		[07/2024] We release <a href = "https://arxiv.org/abs/2407.08683" target="_blank">SEED-Story</a> for Multimodal Long Story Generation based on SEED-X.
	</li>
				<li>
		[04/2024] We release <a href = "https://arxiv.org/abs/2404.14396" target="_blank">SEED-X</a>, the latest in our SEED series, which unifies multi-granularity comprehension and generation.
	</li>
				<li>
		[02/2024] <a href = "https://github.com/AILab-CVC/SEED-Bench" target="_blank">SEED-Bench</a> is accepted by CVPR 2024.
	</li>
			<li>
		[01/2024] <a href = "https://arxiv.org/abs/2310.01218" target="_blank">SEED-LLaMA</a> is accepted by ICLR 2024.
	</li>
		<li>
		[12/2023] We release  <a href = "https://chenyi99.github.io/ego_plan" target="_blank">EgoPlan-Bench</a>, which evaluates egocentric embodied planning of MLLMs.
		</li>
		<li>
		[11/2023] We release  <a href = "https://github.com/AILab-CVC/SEED-Bench" target="_blank">SEED-Bench-2</a>, evaluating the hierarchical capabilities of MLLMs.
		</li>
		<li>
		[10/2023] We release an online gradio demo of <a href = "https://10a4e7976e6fc2032c.gradio.live/" target="_blank">SEED-LLaMA</a>.
		</li>
		<li>
		[10/2023] We release the technical report of <a href = "https://arxiv.org/abs/2310.01218" target="_blank">SEED-LLaMA</a>, which is empowered by the improved SEED-2 tokenizer.
	</li>
		<li>
		[08/2023] We release <a href = "https://github.com/AILab-CVC/SEED-Bench" target="_blank">SEED-Bench</a>, the most comprehensive MLLM benchmark to date.
	</li>
		<li>
		[07/2023] We release our <a href = "https://github.com/AILab-CVC/SEED" target="_blank">SEED</a>. Stay tuned for more updates.
	</li>
	<li>
		[02/2023] Three papers were accepted by CVPR 2023.
	</li>
	<li>
		[07/2022] One paper was accepted by ECCV 2022.
	</li>
	<li>
		[03/2022] One paper was accepted by CVPR 2022 as oral.
	</li>
	<li>
		[11/2021] One paper was accepted by IEEE TIP.
	</li>
	<li>
		[03/2021] Two papers were accepted by CVPR 2021.
	</li>
	<li>
		[03/2019] One paper was accepted by CVPR 2019.
	</li>
</ul>



<h2>SEED Series</h2>
<!-- 插入图片和文字 -->
<div>
  <img src="./pic/paper/SEED_series_v2.png" alt="Description of the image" style="width:100%; max-width:1200px;">
  <p><br>Since July 2023, our SEED Series has been pioneering the exploration of Multimodal Foundation Models for Unified Comprehension and Generation,
	  along with developing cutting-edge Benchmarks. We continuously explore real-world applications of unified foundation models, expand into broader modalities,
	  and enhance advanced capabilities like multimodal interleaved generation, and reasoning. All models and data are open-sourced.
</p>
</div>

<h2> Selected Publications</h2>
( *equal contribution   <sup>#</sup>corresponding author / project lead )
<ul>
	<li>
		ARC-Hunyuan-Video-7B: Structured Video Comprehension of Real-World Shorts <br />
		<b>Yuying Ge</b>, Yixiao Ge, Chen Li, Teng Wang, Junfu Pu, Yizhuo Li, Lu Qiu, Jin Ma, Lisheng Duan, Xinyu Zuo, Jinwen Luo, Weibo Gu, Zexuan Li, Xiaojing Zhang, Yangyu Tao, Han Hu, Di Wang, Ying Shan
		<br /> arXiv preprint, 2025.
		[<a href="https://arxiv.org/abs/2507.20939">Paper</a>]
		[<a href="https://tencentarc.github.io/posts/arc-video-announcement/">Project</a>]
		[<a href="https://github.com/TencentARC/ARC-Hunyuan-Video-7B">Code</a>]
		[<a href="https://huggingface.co/TencentARC/ARC-Hunyuan-Video-7B">Model</a>]
		<img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/TencentARC/ARC-Hunyuan-Video-7B?style=social">
		<br />
	  </li>
	<li>
		GRPO-CARE: Consistency-Aware Reinforcement Learning for Multimodal Reasoning <br />
		Yi Chen, <b>Yuying Ge<sup>#</sup></b>, Rui Wang, Yixiao Ge, Junhao Cheng, Ying Shan, Xihui Liu
		<br /> arXiv preprint, 2025.
		[<a href="https://arxiv.org/abs/2506.16141">Paper</a>]
		[<a href="https://github.com/TencentARC/GRPO-CARE">Code</a>]
		[<a href="https://huggingface.co/TencentARC/GRPO-CARE">Model</a>]
		<img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/TencentARC/GRPO-CARE?style=social">
		<br />
	  </li>
	<li>
		Aligning Latent Spaces with Flow Priors <br />
		Yizhuo Li,  <b>Yuying Ge<sup>#</sup></b>, Yixiao Ge, Ying Shan, Ping Luo
		<br /> arXiv preprint, 2025.
		[<a href="https://arxiv.org/abs/2506.05240">Paper</a>]
		[<a href="https://github.com/liyz15/Aligning-Latent-Spaces-with-Flow-Priors">Code</a>]
		[<a href="https://liyizhuo.com/align/">Project</a>]
		<img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/liyz15/Aligning-Latent-Spaces-with-Flow-Priors?style=social">
		<br />
	  </li>
	<li>
		AnimeShooter: A Multi-Shot Animation Dataset for Reference-Guided Video Generation <br />
		Lu Qiu, Yizhuo Li, <b>Yuying Ge<sup>#</sup></b>, Yixiao Ge, Ying Shan, Xihui Liu
		<br /> arXiv preprint, 2025.
		[<a href="https://arxiv.org/abs/2506.03126">Paper</a>]
		[<a href="https://github.com/qiulu66/Anime-Shooter">Code</a>]
		[<a href="https://qiulu66.github.io/animeshooter/">Project</a>]
		<img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/qiulu66/Anime-Shooter?style=social">
		<br />
	  </li>
	<li>
		Video-Holmes: Can MLLM Think Like Holmes for Complex Video Reasoning? <br />
		Junhao Cheng, <b>Yuying Ge<sup>#</sup></b>, Teng Wang, Yixiao Ge, Jing Liao, Ying Shan
		<br /> arXiv preprint, 2025.
		[<a href="https://arxiv.org/abs/2505.21374">Paper</a>]
		[<a href="https://github.com/TencentARC/Video-Holmes">Code</a>]
		[<a href="https://video-holmes.github.io/Page.github.io/">Project</a>]
		<img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/TencentARC/Video-Holmes?style=social">
		<br />
	  </li>
	<li>
		AnimeGamer: Infinite Anime Life Simulation with Next Game State Prediction <br />
		Junhao Cheng, <b>Yuying Ge<sup>#</sup></b>, Yixiao Ge, Jing Liao, Ying Shan
		<br /> ICCV, 2025.
		[<a href="https://arxiv.org/abs/2504.01014">Paper</a>]
		[<a href="https://github.com/TencentARC/AnimeGamer">Code</a>]
		[<a href="https://howe125.github.io/AnimeGamer.github.io/">Project</a>]
		<img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/TencentARC/AnimeGamer?style=social">
		<br />
	  </li>
	<li>
		Exploring the Effect of Reinforcement Learning on Video Understanding: Insights from SEED-Bench-R1 <br />
		Yi Chen, <b>Yuying Ge<sup>#</sup></b>, Rui Wang, Yixiao Ge, Lu Qiu, Ying Shan, Xihui Liu
		<br /> arXiv preprint, 2025.
		[<a href="https://arxiv.org/abs/2503.24376">Paper</a>]
		[<a href="https://github.com/TencentARC/SEED-Bench-R1">Code</a>]
		[<a href="https://huggingface.co/datasets/TencentARC/SEED-Bench-R1">Dataset</a>]
		<img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/TencentARC/SEED-Bench-R1?style=social">
		<br />
	  </li>
	<li>
		GenHancer: Imperfect Generative Models are Secretly Strong Vision-Centric Enhancers <br />
		Shijie Ma, <b>Yuying Ge<sup>#</sup></b>, Teng Wang, Yuxin Guo, Yixiao Ge, Ying Shan
		<br /> ICCV, 2025.
		[<a href="https://arxiv.org/abs/2503.19480">Paper</a>]
		[<a href="https://github.com/mashijie1028/GenHancer">Code</a>]
		[<a href="https://mashijie1028.github.io/GenHancer/">Project</a>]
		<img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/mashijie1028/GenHancer?style=social">
		<br />
	  </li>
	<li>
		Divot: Diffusion Powers Video Tokenizer for Comprehension and Generation <br />
		<b>Yuying Ge</b>, Yizhuo Li, Yixiao Ge, Ying Shan
		<br /> CVPR, 2025.
		[<a href="https://arxiv.org/abs/2412.04432">Paper</a>]
		[<a href="https://github.com/TencentARC/Divot">Code</a>]
		<img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/TencentARC/Divot?style=social">
		<br />
	  </li>
	<li>
		DiCoDe: Diffusion-Compressed Deep Tokens for Autoregressive Video Generation with Language Models <br />
		Yizhuo Li, <b>Yuying Ge<sup>#</sup></b>, Yixiao Ge, Ping Luo, Ying Shan
		<br /> arXiv preprint, 2024.
		[<a href="https://arxiv.org/abs/2412.04446">Paper</a>]
		[<a href="https://liyizhuo.com/DiCoDe/">Project</a>]
		[<a href="https://github.com/liyz15/Diffusion-Compressed-Deep-Tokens">Code</a>]
		<img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/liyz15/Diffusion-Compressed-Deep-Tokens?style=social">
		<br />
	  </li>
	<li>
		Moto: Latent Motion Token as the Bridging Language for Robot Manipulation <br />
		Yi Chen, <b>Yuying Ge<sup>#</sup></b>, Yizhuo Li, Yixiao Ge, Mingyu Ding, Ying Shan, Xihui Liu
		<br /> ICCV, 2025.
		[<a href="https://arxiv.org/pdf/2412.04445">Paper</a>]
		[<a href="https://github.com/TencentARC/Moto/">Code</a>]
		[<a href="https://chenyi99.github.io/moto/">Project</a>]
		<img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/TencentARC/Moto?style=social">
		<br />
	  </li>
	<li>
		EgoPlan-Bench2: A Benchmark for Multimodal Large Language Model Planning in Real-World Scenarios <br />
		Lu Qiu, <b>Yuying Ge<sup>#</sup></b>, Yi Chen, Yixiao Ge, Ying Shan, Xihui Liu
		<br /> arXiv preprint, 2024.
		[<a href="https://arxiv.org/abs/2412.04447">Paper</a>]
		[<a href="https://github.com/qiulu66/EgoPlan-Bench2/">Code</a>]
		[<a href="https://qiulu66.github.io/egoplanbench2/">Project</a>]
		<img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/qiulu66/EgoPlan-Bench2?style=social">
		<br />
	  </li>
	<li>
		SEED-Story: Multimodal Long Story Generation with Large Language Model <br />
		Shuai Yang, <b>Yuying Ge<sup>#</sup></b>, Yang Li, Yukang Chen, Yixiao Ge, Ying Shan, Yingcong Chen
		<br /> arXiv preprint, 2024.
		[<a href="https://arxiv.org/abs/2407.08683">Paper</a>]
		[<a href="https://github.com/TencentARC/SEED-Story">Code</a>]
		<img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/TencentARC/SEED-Story?style=social">
		<br />
	  </li>
	<li>
		EgoPlan-Bench: Benchmarking Multimodal Large Language Models for Human-Level Planning <br />
		Yi Chen, <b>Yuying Ge<sup>#</sup></b>, Yixiao Ge, Mingyu Ding, Bohao Li, Rui Wang, Ruifeng Xu, Ying Shan, Xihui Liu
		<br /> arXiv preprint, 2024.
		[<a href="https://arxiv.org/abs/2404.14396">Paper</a>]
		[<a href="https://github.com/ChenYi99/EgoPlan">Code</a>]
		[<a href="https://chenyi99.github.io/ego_plan/">Project</a>]
		<img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/ChenYi99/EgoPlan?style=social">
		<br />
	  </li>
	<li>
		SEED-X: Multimodal Models with Unified Multi-granularity Comprehension and Generation <br />
		<b>Yuying Ge*</b>, Sijie Zhao*, Jinguo Zhu*, Yixiao Ge, Kun Yi, Lin Song, Chen Li, Xiaohan Ding, Ying Shan
		<br /> arXiv preprint, 2024.
		[<a href="https://arxiv.org/abs/2404.14396">Paper</a>]
		[<a href="https://github.com/AILab-CVC/SEED-X">Code</a>]
		[<a href="https://arc.tencent.com/en/ai-demos/multimodal">Demo</a>]
		<img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/AILab-CVC/SEED-X?style=social">
		<br />
	  </li>
	  <li>
		SEED-Bench: Benchmarking Multimodal Large Language Models <br />
		Bohao Li*, <b>Yuying Ge*</b>, Yixiao Ge, Guangzhi Wang, Rui Wang, Ruimao Zhang, Ying Shan
		<br /> CVPR, 2024.
		[<a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Li_SEED-Bench_Benchmarking_Multimodal_Large_Language_Models_CVPR_2024_paper.pdf">Paper</a>]
		[<a href="https://github.com/AILab-CVC/SEED-Bench">Code</a>]
		[<a href="https://huggingface.co/datasets/AILab-CVC/SEED-Bench-2">Data</a>]
		[<a href="https://huggingface.co/spaces/AILab-CVC/SEED-Bench_Leaderboard">Leaderboard</a>]
		<img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/AILab-CVC/SEED-Bench?style=social">
		<br />
	  </li>
	  <li>
		Making LLaMA SEE and Draw with SEED Tokenizer <br />
		<b>Yuying Ge*</b>, Sijie Zhao*, Ziyun Zeng, Yixiao Ge, Chen Li, Xintao Wang, Ying Shan
		<br /> ICLR, 2024.
		[<a href="https://openreview.net/forum?id=0Nui91LBQS">Paper</a>]
		[<a href="https://github.com/AILab-CVC/SEED/tree/main">Code</a>]
		<img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/AILab-CVC/SEED?style=social">
		<br />
	  </li>
	  <li>
		Planting a SEED of Vision in Large Language Model <br />
		<b>Yuying Ge*</b>, Yixiao Ge*, Ziyun Zeng, Xintao Wang, Ying Shan
		<br /> Technical Report, 2023.
		[<a href="https://arxiv.org/abs/2307.08041">Paper</a>]
		[<a href="https://github.com/AILab-CVC/SEED/tree/v1">Code</a>]
		<img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/AILab-CVC/SEED?style=social">
		<br />
	  </li>
	  <li>
		Policy Adaptation from Foundation Model Feedback <br />
		<b>Yuying Ge</b>, Annabella Macaluso, Li Erran Li, Ping Luo, Xiaolong Wang
		<br /> CVPR, 2023.
		[<a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Ge_Policy_Adaptation_From_Foundation_Model_Feedback_CVPR_2023_paper.pdf">Paper</a>]
		[<a href="ttps://geyuying.github.io/PAFF/">Project</a>]
		[<a href="https://github.com/geyuying/PAFF_code">Code</a>]
		<img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/geyuying/PAFF_code?style=social">
		<br />
	  </li>
	  <li>
		Learning Transferable Spatiotemporal Representations from Natural Script Knowledge <br />
		Ziyun Zeng*, <b>Yuying Ge*</b>, Xihui Liu, Bin Chen, Ping Luo, Shu-Tao Xia, Yixiao Ge
		<br /> CVPR, 2023.
		[<a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zeng_Learning_Transferable_Spatiotemporal_Representations_From_Natural_Script_Knowledge_CVPR_2023_paper.pdf">Paper</a>]
		[<a href="https://github.com/TencentARC/TVTS">Code</a>]
		<img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/TencentARC/TVTS?style=social">
		<br />
	  </li>
	  <li>
		MILES: Visual BERT Pre-training with Injected Language Semantics for Video-text Retrieval <br />
		<b>Yuying Ge</b>, Yixiao Ge, Xihui Liu, Alex Jinpeng Wang, Jianping Wu, Ying Shan, Xiaohu Qie and Ping Luo
		<br /> ECCV, 2022.
		[<a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136950685.pdf">Paper</a>]
		[<a href="https://github.com/TencentARC/MCQ/blob/main/MILES.md">Code</a>]
		<img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/TencentARC/MCQ?style=social">
		<br />
	  </li>
	  <li>
		Bridging Video-text Retrieval with Multiple Choice Questions <br />
		<b>Yuying Ge</b>, Yixiao Ge, Xihui Liu, Dian Li, Ying Shan, Xiaohu Qie and Ping Luo</i>
		<br /> CVPR, 2022 (oral).
		[<a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Ge_Bridging_Video-Text_Retrieval_With_Multiple_Choice_Questions_CVPR_2022_paper.pdf">Paper</a>]
		[<a href="MCQ.html">Project</a>]
		[<a href="https://github.com/TencentARC/MCQ">Code</a>]
		<img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/TencentARC/MCQ?style=social">
		<br />
	  </li>
	  <li>
		Parser-Free Virtual Try-on via Distilling Appearance Flows <br />
		<b>Yuying Ge</b>, Yibing Song, Ruimao Zhang, Chongjian Ge, Wei Liu, and Ping Luo
		<br /> CVPR, 2021.
		[<a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Ge_Parser-Free_Virtual_Try-On_via_Distilling_Appearance_Flows_CVPR_2021_paper.pdf">Paper</a>]
		[<a href="https://github.com/geyuying/PF-AFN">Code</a>]
		<img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/geyuying/PF-AFN?style=social">
		<br />
	  </li>
	  <li>
		DeepFashion2: A Versatile Benchmark for Detection, Pose Estimation, Segmentation and Re-Identification of Clothing Images <br />
		<b>Yuying Ge</b>, Ruimao Zhang, Xiaogang Wang, Xiaoou Tang, and Ping Luo
		<br /> CVPR, 2019.
		[<a href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Ge_DeepFashion2_A_Versatile_Benchmark_for_Detection_Pose_Estimation_Segmentation_and_CVPR_2019_paper.pdf">Paper</a>]
		[<a href="https://github.com/switchablenorms/DeepFashion2">Data</a>]
		<img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/switchablenorms/DeepFashion2?style=social">
		<br />
	  </li>
	</ul>





</tbody></table>

	<h2><font> Education </font></h2>
<ul style="list-style-type:none">
	  <p style="margin-left: 0px; line-height: 150%; margin-top: 8px; margin-bottom: 8px;"><font size="3"><meta charset="utf-8">
		  Ph.D., Department of Computer Science, The University of Hong Kong, 2019 - 2023<br>
		  Bachelor, University of Electronic Science and Technology of China (UESTC) (ranking 1/525), 2014 - 2018<br>
	  </font> </p>
</ul>

	<h2><font> Experiences </font></h2>
<ul style="list-style-type:none">
	  <p style="margin-left: 0px; line-height: 150%; margin-top: 8px; margin-bottom: 8px;"><font size="3"><meta charset="utf-8">
		  Staff Research Scientist at XPENG Robotics, 2025 - Present<br>
		  Senior Researcher in Tencent ARC Lab, 2024 - 2025<br>
		  Senior Researcher in Tencent AI Lab, 2023 - 2024<br>
		  Intern in Tencent ARC Lab, 2021 - 2022 <br>
		  Intern in Tencent AI Lab, 2020 - 2021 <br>
		  Research Assistant in Multimedia Lab (MMLab), The Chinese University of Hong Kong, 2018 - 2019 <br>
		  Intern in SenseTime Research, 2017 - 2018 <br>

	  </font> </p>
</ul>

<h2><font> Academic Activities </font></h2>
<ul style="list-style-type:none">
	  <p style="margin-left: 0px; line-height: 150%; margin-top: 8px; margin-bottom: 8px;"><font size="3"><meta charset="utf-8">
		  Reviewer for CVPR, ICLR, ICML, NeurIPS, ECCV, ICCV, TPAMI, TNNLS, TMM, TVCJ<br>
		  Organizer of DeepFashion2 Challenge <a href='https://competitions.codalab.org/competitions/22966'>Clothes Landmark Detection</a>
			  and <a href='https://competitions.codalab.org/competitions/22967'>Clothes Retrieval</a> in 2019, 2020<br>
		  Organizer of <a href='https://sites.google.com/view/cvcreative2020'>Third Workshop on Computer Vision for Fashion, Art and Design</a> in CVPR, 2020 <br>
		  Organizer of <a href='https://sites.google.com/view/cvcreative/home?authuser=0'>Second Workshop on Computer Vision for Fashion, Art and Design</a> in ICCV, 2019 <br>
	  </font> </p>
</ul>




<p align=right>
	<a class="pull-right" href="#">
	<script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=zPFGAwps3dHeHg_L_f7k-PGV37THUtOVbgXT6ZedWOs&cl=ffffff&w=a"></script></center>
	</a>
</p>

<p><center><font>
        <br>&copy; Yuying Ge | Last updated: Dec. 2021</font></center>
</p>

</div>
</body></html>
