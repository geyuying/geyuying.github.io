<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<link rel="shortcut icon" href="myIcon.ico">
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />

<meta name="keywords" content="Yuying Ge, CS, HKU, The University of Hong Kong">
<meta name="description" content="Yuying Ge&#39;s home page">
<link rel="stylesheet" href="jemdoc.css" type="text/css">
<title>Yuying Ge&#39;s Homepage</title>
<!-- Google Analytics -->
<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-159069803-1', 'auto');
ga('send', 'pageview');
</script>
<!-- End Google Analytics -->
<!--
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-87320911-1', 'auto');
  ga('send', 'pageview');

</script>
-->
</head>
<body>

<div id="layout-content" style="margin-top:25px">
<table>
	<tbody>
		<tr>
			<td width="670">
				<div id="toptitle">
					<h1>Yuying Ge <font face="Arial">    葛玉莹 </font></h1></div>
				<img src="./pic/others/email.png" height="22px">  yyge13@gmail.com<br>
				<br>
				<img src="./pic/others/google_scholar_logo.png" height="22px">  <a href="https://scholar.google.com/citations?user=hv1LiiEAAAAJ&hl=en">Google Sholar</a><br>
				<br>
				<img src="./pic/others/github_logo.png" height="22px">  <a href="https://github.com/geyuying">Github</a><br>
				<br>
				<img src="./pic/others/location.png" height="22px"> Shenzhen, China<br>

			</td>
			<td>
    <img src="./pic/yuyingge.jpg" border="0" width="180"><br>
   </td>
		</tr><tr>
	</tr></tbody>
</table>

<!--<h2>Biography [<a href="./CV-JinYueming.pdf">CV</a>]</h2>-->
<h2>Biography </h2>
<p>
	I am currently a Researcher at <a href="https://ai.tencent.com/ailab/en/index/">Tencent AI Lab</a>, working on multimodal foundation models.
	In Aug 2023, I got my Ph.D. degree from the Department of Computer Science, The University of Hong Kong,
	under the supervision of <a href="http://luoping.me/">Prof. Ping Luo</a>.
	 I was also a visiting student at UCSD, working with <a href="https://xiaolonw.github.io/">Prof. Xiaolong Wang</a>.
	We are actively looking for self-motivated interns to work on related research topics. Please feel free to reach out if you are interested.

	<br>
</p>


<h2>News</h2>
<ul>
		<li>
		[11/2023] We release  <a href = "https://github.com/AILab-CVC/SEED-Bench" target="_blank">SEED-Bench-2</a>, evaluating the hierarchical capabilities of MLLMs.
		</li>
		<li>
		[10/2023] We release an online gradio demo of <a href = "https://10a4e7976e6fc2032c.gradio.live/" target="_blank">SEED-LLaMA</a>.
		</li>
		<li>
		[10/2023] We release the technical report of <a href = "https://arxiv.org/abs/2310.01218" target="_blank">SEED-LLaMA</a>, which is empowered by the improved SEED-2 tokenizer.
	</li>
		<li>
		[08/2023] We release <a href = "https://github.com/AILab-CVC/SEED-Bench" target="_blank">SEED-Bench</a>, the most comprehensive MLLM benchmark to date.
	</li>
		<li>
		[07/2023] We release our <a href = "https://github.com/AILab-CVC/SEED" target="_blank">SEED</a>. Stay tuned for more updates.
	</li>
	<li>
		[02/2023] Three papers were accepted by CVPR 2023.
	</li>	
	<li>
		[07/2022] One paper was accepted by ECCV 2022.
	</li>	
	<li>
		[03/2022] One paper was accepted by CVPR 2022 as oral.
	</li>
	<li>
		[11/2021] One paper was accepted by IEEE TIP.
	</li>
	<li>
		[03/2021] Two papers were accepted by CVPR 2021.
	</li>
	<li>
		[03/2019] One paper was accepted by CVPR 2019.
	</li>
</ul>




<h2> Publications</h2>
<table id="tbPublications" width="100%">
	<tbody>
					<tr>
	<td><center><img width="200" src="./pic/paper/seed-bench-2.jpeg"></center></td>
	<td>
		<font size="2">SEED-Bench-2: Benchmarking Multimodal Large Language Models,
		<br>
		<i>Bohao Li*, <b>Yuying Ge*</b>, Yixiao Ge, Guangzhi Wang, Rui Wang, Ruimao Zhang, Ying Shan, </i>
		<br>
		arXiv preprint, 2023
		<br>
			[<a href='https://arxiv.org/abs/2311.17092' target="_blank"><b>paper</b></a>|<a href='https://github.com/AILab-CVC/SEED-Bench' target="_blank"><b>code</b></a>|<a href='https://huggingface.co/datasets/AILab-CVC/SEED-Bench-2' target="_blank"><b>dataset</b></a>|<a href='https://huggingface.co/spaces/AILab-CVC/SEED-Bench_Leaderboard' target="_blank"><b>leaderboard</b></a>]
		</td>		
					<tr>
	<td><center><img width="200" src="./pic/paper/cookie.jpg"></center></td>
	<td>
		<font size="2">Making LLaMA SEE and Draw with SEED Tokenizer,
		<br>
		<i><b>Yuying Ge*</b>, Sijie Zhao*, Ziyun Zeng, Yixiao Ge, Chen Li, Xintao Wang, Ying Shan, </i>
		<br>
		arXiv preprint, 2023
		<br>
			[<a href='https://arxiv.org/abs/2310.01218' target="_blank"><b>paper</b></a>|<a href='https://github.com/AILab-CVC/SEED/tree/main' target="_blank"><b>code</b></a>|<a href='https://ailab-cvc.github.io/seed/' target="_blank"><b>project</b></a>|<a href='https://10a4e7976e6fc2032c.gradio.live/' target="_blank"><b>gradio demo</b></a>]
		</td>

					<tr>
	<td><center><img width="200" src="./pic/paper/seed.jpg"></center></td>
	<td>
		<font size="2">Planting a SEED of Vision in Large Language Model,
		<br>
		<i><b>Yuying Ge*</b>, Yixiao Ge*, Ziyun Zeng, Xintao Wang, Ying Shan, </i>
		<br>
		Technical Report, 2023
		<br>
			[<a href='https://arxiv.org/abs/2307.08041' target="_blank"><b>paper</b></a>|<a href='https://github.com/AILab-CVC/SEED/tree/v1' target="_blank"><b>code</b></a>]
		</td>

						
							<tr>
	<td><center><img width="250" src="./pic/paper/sim2real.png"></center></td>
	<td>
		<font size="2">Policy Adaptation from Foundation Model Feedback,
		<br>
		<i><b>Yuying Ge</b>, Annabella Macaluso, Li Erran Li, Ping Luo, Xiaolong Wang</i>
		<br>
		IEEE/CVF Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>) 2023
		<br>
			[<a href='https://arxiv.org/abs/2212.07398' target="_blank"><b>paper</b></a>|<a href='https://geyuying.github.io/PAFF/' target="_blank"><b>project</b></a>]
		</td>
		
		
			<tr>
	<td><center><img width="200" src="./pic/paper/TVTS.jpeg"></center></td>
	<td>
		<font size="2">Learning Transferable Spatiotemporal Representations from Natural Script Knowledge,
		<br>
		<i>Ziyun Zeng*, <b>Yuying Ge*</b>, Xihui Liu, Bin Chen, Ping Luo, Shu-Tao Xia, Yixiao Ge</i>
		<br>
		IEEE/CVF Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>) 2023
		<br>
			[<a href='https://arxiv.org/abs/2209.15280' target="_blank"><b>paper</b></a>|<a href='https://github.com/TencentARC/TVTS' target="_blank"><b>code</b></a>]
		</td>

		<tr>
	<td><center><img width="250" src="./pic/paper/MILES.jpg"></center></td>
	<td>
		<font size="2">MILES: Visual BERT Pre-training with Injected Language Semantics for Video-text Retrieval,
		<br>
		<i><b>Yuying Ge</b>, Yixiao Ge, Xihui Liu, Alex Jinpeng Wang, Jianping Wu, Ying Shan, Xiaohu Qie and Ping Luo</i>
		<br>
		European Conference on Computer Vision (<b>ECCV</b>) 2022
		<br>
			[<a href='https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136950685.pdf' target="_blank"><b>paper</b></a>|<a href='https://github.com/TencentARC/MCQ/blob/main/MILES.md' target="_blank"><b>code</b></a>]
		</td>

	</tr>

	<tr>
	<td><center><img width="250" src="./pic/paper/MCQ.jpg"></center></td>
	<td>
		<font size="2">Bridging Video-text Retrieval with Multiple Choice Questions,
		<br>
		<i><b>Yuying Ge</b>, Yixiao Ge, Xihui Liu, Dian Li, Ying Shan, Xiaohu Qie and Ping Luo</i>
		<br>
		IEEE/CVF Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>) 2022 (<b>oral</b>)
		<br>
		[<a href='https://openaccess.thecvf.com/content/CVPR2022/papers/Ge_Bridging_Video-Text_Retrieval_With_Multiple_Choice_Questions_CVPR_2022_paper.pdf' target="_blank"><b>paper</b></a>|<a href='https://github.com/TencentARC/MCQ' target="_blank"><b>code</b></a>|<a href='MCQ.html' target="_blank"><b>project</b></a>]
		</td>

	</tr>

		<tr>
	<td><center><img width="250" src="./pic/paper/metadance.jpg"></center></td>
	<td>
		<font size="2">MetaDance: Few-shot Dancing Video Retargeting via Temporal-aware Meta-learning,
		<br>
		<i><b>Yuying Ge</b>, Yibing Song, Ruimao Zhang and Ping Luo</i>
		<br>
		arXiv preprint, 2022
		<br>
		[<a href='https://arxiv.org/abs/2201.04851' target="_blank"><b>paper</b></a>|<a href='https://github.com/geyuying/MetaDance' target="_blank"><b>demo</b></a>]
		</td>

	</tr>
	<tr>
	<td><center><img width="250" src="./pic/paper/metacloth.jpg"></center></td>
	<td>
		<font size="2">MetaCloth: Learning Unseen Tasks of Dense Fashion Landmark Detection from a Few Samples,
		<br>
		<i><b>Yuying Ge</b>, Ruimao Zhang, and Ping Luo</i>
		<br>
		IEEE Transactions on Image Processing (<b>TIP</b>) 2021
		<br>
		[<a href='https://arxiv.org/abs/2112.02763' target="_blank"><b>paper</b></a>]
		</td>

	</tr>

	<tr>
	<td><center><img width="250" src="./pic/paper/cvpr2021_pfafn.png"></center></td>
	<td>
		<font size="2">Parser-Free Virtual Try-on via Distilling Appearance Flows,
		<br>
		<i><b>Yuying Ge</b>, Yibing Song, Ruimao Zhang, Chongjian Ge, Wei Liu, and Ping Luo</i>
		<br>
		IEEE/CVF Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>) 2021
		<br>
		[<a href='https://openaccess.thecvf.com/content/CVPR2021/papers/Ge_Parser-Free_Virtual_Try-On_via_Distilling_Appearance_Flows_CVPR_2021_paper.pdf' target="_blank"><b>paper</b></a>|<a href='https://github.com/geyuying/PF-AFN' target="_blank"><b>code</b></a>]
		</td>

	</tr>

	<!--<tr>
		<td><center><img width="250" src="./pic/paper/cvpr2021_dcton.png"></center></td>
		<td>
			<font size="2">Disentangled Cycle Consistency for Highly-realistic Virtual Try-On,
			<br>
			<i>Chongjian Ge, Yibing Song, <b>Yuying Ge</b>, Han Yang, Wei Liu, and Ping Luo</i>
			<br>
			IEEE/CVF Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>) 2021
			<br>
			[<a href='https://arxiv.org/abs/2103.09479' target="_blank"><b>paper</b></a>|<a href='https://github.com/ChongjianGE/DCTON' target="_blank"><b>code</b></a>]
		</td>
	</tr>-->

	<tr>
	<td><center><img width="250" src="./pic/paper/deepfashion2.jpg"></center></td>
	<td>
		<font size="2">DeepFashion2: A Versatile Benchmark for Detection, Pose Estimation, Segmentation and Re-Identification of Clothing Images,
		<br>
		<i><b>Yuying Ge</b>, Ruimao Zhang, Xiaogang Wang, Xiaoou Tang, and Ping Luo</i>
		<br>
		IEEE/CVF Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>) 2019
		<br>
		[<a href='https://openaccess.thecvf.com/content_CVPR_2019/papers/Ge_DeepFashion2_A_Versatile_Benchmark_for_Detection_Pose_Estimation_Segmentation_and_CVPR_2019_paper.pdf'  target="_blank"><b>paper</b></a>|<a href='https://github.com/switchablenorms/DeepFashion2' target="_blank"><b>dataset</b></a>]
		</td>




</tbody></table>

	<h2><font> Education </font></h2>
<ul style="list-style-type:none">
	  <p style="margin-left: 0px; line-height: 150%; margin-top: 8px; margin-bottom: 8px;"><font size="3"><meta charset="utf-8">
		  Ph.D., Department of Computer Science, The University of Hong Kong, 2019 - 2023<br>
		  Bachelor, University of Electronic Science and Technology of China (UESTC) (ranking 1/525), 2014 - 2018<br>
	  </font> </p>
</ul>

	<h2><font> Experiences </font></h2>
<ul style="list-style-type:none">
	  <p style="margin-left: 0px; line-height: 150%; margin-top: 8px; margin-bottom: 8px;"><font size="3"><meta charset="utf-8">
		  Senior Researcher in Tencent AI Lab, 2023 - Present<br>
		  Intern in Tencent ARC Lab, 2021 - 2022 <br>
		  Intern in Tencent AI Lab, 2020 - 2021 <br>
		  Research Assistant in Multimedia Lab (MMLab), The Chinese University of Hong Kong, 2018 - 2019 <br>
		  Intern in SenseTime Research, 2017 - 2018 <br>

	  </font> </p>
</ul>

<h2><font> Academic Activities </font></h2>
<ul style="list-style-type:none">
	  <p style="margin-left: 0px; line-height: 150%; margin-top: 8px; margin-bottom: 8px;"><font size="3"><meta charset="utf-8">
		  Reviewer for CVPR, ICML, NeurIPS, ECCV, ICCV, TPAMI, TNNLS, TMM, TVCJ<br>
		  Organizer of DeepFashion2 Challenge <a href='https://competitions.codalab.org/competitions/22966'>Clothes Landmark Detection</a>
			  and <a href='https://competitions.codalab.org/competitions/22967'>Clothes Retrieval</a> in 2019, 2020<br>
		  Organizer of <a href='https://sites.google.com/view/cvcreative2020'>Third Workshop on Computer Vision for Fashion, Art and Design</a> in CVPR, 2020 <br>
		  Organizer of <a href='https://sites.google.com/view/cvcreative/home?authuser=0'>Second Workshop on Computer Vision for Fashion, Art and Design</a> in ICCV, 2019 <br>
	  </font> </p>
</ul>




<p align=right>
	<a class="pull-right" href="#">
	<script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=zPFGAwps3dHeHg_L_f7k-PGV37THUtOVbgXT6ZedWOs&cl=ffffff&w=a"></script></center>
	</a>
</p>

<p><center><font>
        <br>&copy; Yuying Ge | Last updated: Dec. 2021</font></center>
</p>

</div>
</body></html>
